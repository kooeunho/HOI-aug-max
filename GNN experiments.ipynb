{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64090fc6-f698-475c-99d2-6574d866871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Coauthor\n",
    "from torch_geometric.utils import to_networkx, to_scipy_sparse_matrix\n",
    "from torch_geometric.nn import GATConv, SGConv, GCNConv\n",
    "from collections import Counter\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Basic Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def sample_per_class_fixed(labels, candidate_idx, num_classes, k_per_class):\n",
    "    \"\"\"Samples a fixed number of nodes per class from a candidate set.\"\"\"\n",
    "    sampled_indices = []\n",
    "    for c in range(num_classes):\n",
    "        class_mask = (labels[candidate_idx] == c)\n",
    "        class_indices = candidate_idx[class_mask]\n",
    "        num_to_sample = min(k_per_class, class_indices.numel())\n",
    "        if num_to_sample > 0:\n",
    "            perm = torch.randperm(class_indices.numel())\n",
    "            sampled_indices.append(class_indices[perm[:num_to_sample]])\n",
    "    return torch.cat(sampled_indices) if sampled_indices else torch.tensor([], dtype=torch.long)\n",
    "\n",
    "def calculate_all_metrics(pred, prob, labels, mask):\n",
    "    \"\"\"Calculates all specified evaluation metrics.\"\"\"\n",
    "    labels, mask = labels.cpu(), mask.cpu()\n",
    "    mask_indices = mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    pred_tensor = pred.cpu() if torch.is_tensor(pred) else torch.from_numpy(pred).cpu()\n",
    "    prob_tensor = prob.cpu() if torch.is_tensor(prob) else torch.from_numpy(prob).cpu()\n",
    "\n",
    "    y_true = labels[mask_indices].numpy()\n",
    "    y_pred = pred_tensor[mask_indices].numpy()\n",
    "    y_prob = prob_tensor[mask_indices].numpy()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = 0.5 \n",
    "\n",
    "    return {'acc': acc, 'f1': f1, 'auc': auc, 'kappa': kappa}\n",
    "\n",
    "# ====================================================================\n",
    "# 1. GNN MODEL DEFINITIONS (7 Models)\n",
    "# ====================================================================\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid, self.in_head, self.out_head = 8, 8, 1\n",
    "        self.conv1 = GATConv(num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid * self.in_head, num_classes, concat=False, heads=self.out_head, dropout=0.6)\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class SGC(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv = SGConv(num_features, num_classes, K=2, cached=False)\n",
    "    def forward(self, data):\n",
    "        return F.log_softmax(self.conv(data.x, data.edge_index), dim=1)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "        return F.log_softmax(self.conv2(x, data.edge_index), dim=1)\n",
    "\n",
    "class PlanetoidGCN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_nodes):\n",
    "        super(PlanetoidGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "        self.embedding = nn.Parameter(torch.randn(num_nodes, 16))\n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "        return F.log_softmax(self.conv2(x, data.edge_index), dim=1)\n",
    "    def get_embedding(self):\n",
    "        return self.embedding\n",
    "\n",
    "class Delta(nn.Module):\n",
    "    def __init__(self, d_in, d_red=6, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.f = nn.Linear(d_in, d_red, bias=True)\n",
    "        self.fk_list = None\n",
    "        self.gate = nn.Sequential(nn.Linear(d_red, 1), nn.Sigmoid())\n",
    "        self.act = act\n",
    "    def set_fk_list(self, K):\n",
    "        self.fk_list = nn.ModuleList([nn.Linear(self.f.in_features, self.f.out_features, bias=True) for _ in range(K)])\n",
    "    def forward(self, H_list):\n",
    "        out = []\n",
    "        for k in range(len(H_list)):\n",
    "            f_hk = self.f(H_list[k])\n",
    "            fk_hk = self.fk_list[k](H_list[k])\n",
    "            beta = self.gate(f_hk * fk_hk)\n",
    "            out.append(self.act(beta * (f_hk - fk_hk)))\n",
    "        return out\n",
    "\n",
    "class MGNNLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, K, d_red=6):\n",
    "        super().__init__()\n",
    "        self.lin_z = nn.Linear(d_in, d_hidden, bias=False)\n",
    "        self.delta = Delta(d_hidden, d_red=d_red)\n",
    "        self.delta.set_fk_list(K)\n",
    "    def agg_messages(self, Atil_k_list, Z):\n",
    "        return [torch.spmm(A, Z) for A in Atil_k_list]\n",
    "    def forward(self, X, Atil, Atil_list):\n",
    "        Z = torch.spmm(Atil, self.lin_z(X))\n",
    "        Hk = self.agg_messages(Atil_list, Z)\n",
    "        Hk_tilde = self.delta(Hk)\n",
    "        return torch.cat(Hk_tilde, dim=1)\n",
    "\n",
    "class MGNN(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, num_cls, K=3, d_red=8):\n",
    "        super().__init__()\n",
    "        self.layer = MGNNLayer(d_in, d_hidden, K=K, d_red=d_red)\n",
    "        self.out = nn.Linear(K * d_red, num_cls)\n",
    "    def forward(self, data):\n",
    "        H = self.layer(data.x, data.adj_norm, data.motif_adjs)\n",
    "        return F.log_softmax(self.out(H), dim=1)\n",
    "\n",
    "class SDMG(nn.Module):\n",
    "    def __init__(self, feat_dim, out_dim=256, heads=2):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(feat_dim, out_dim // heads, heads=heads, dropout=0.2)\n",
    "        self.gat2 = GATConv(out_dim, out_dim // heads, heads=heads, dropout=0.2)\n",
    "    def forward(self, data):\n",
    "        h = F.elu(self.gat1(data.x, data.edge_index))\n",
    "        return self.gat2(h, data.edge_index)\n",
    "\n",
    "class ActOp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.zeros(3))\n",
    "    def forward(self, x):\n",
    "        stack = torch.stack([F.relu(x), torch.tanh(x), x], dim=-1)\n",
    "        w = F.softmax(self.alpha, dim=0)\n",
    "        return torch.einsum(\"...d,d->...\", stack, w)\n",
    "\n",
    "class LayerAggregator(nn.Module):\n",
    "    def __init__(self, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.zeros(num_layers))\n",
    "    def forward(self, hs: list):\n",
    "        H_stack = torch.stack(hs, dim=0)\n",
    "        w = F.softmax(self.alpha, dim=0)\n",
    "        return torch.einsum(\"l,lnd->nd\", w, H_stack)\n",
    "\n",
    "class WholeGraphEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + [hidden_dim] * num_layers\n",
    "        self.layers = nn.ModuleList([GCNConv(dims[i], dims[i+1]) for i in range(num_layers)])\n",
    "        self.acts = nn.ModuleList([ActOp() for _ in range(num_layers)])\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        hs = []\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = layer(h, edge_index)\n",
    "            h = self.acts[l](h)\n",
    "            hs.append(h)\n",
    "        return hs\n",
    "\n",
    "class CSSE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = WholeGraphEncoder(in_dim, hidden_dim, num_layers, dropout)\n",
    "        self.aggregator = LayerAggregator(num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "    def forward(self, data):\n",
    "        hs = self.encoder(data.x, data.edge_index)\n",
    "        h_final = self.aggregator(hs)\n",
    "        return F.log_softmax(self.classifier(h_final), dim=1)\n",
    "    def get_arch_params(self):\n",
    "        return [p for m in [self.encoder.acts, self.aggregator] for p in m.parameters()]\n",
    "    def get_net_params(self):\n",
    "        arch_ids = {id(p) for p in self.get_arch_params()}\n",
    "        return [p for p in self.parameters() if id(p) not in arch_ids]\n",
    "\n",
    "# ====================================================================\n",
    "# 2. GNN TRAINING PIPELINES\n",
    "# ====================================================================\n",
    "\n",
    "def standard_gnn_training(model, data, device, lr, wd, epochs):\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return torch.exp(model(data))\n",
    "\n",
    "def sample_context(graph, labels, num_samples=100):\n",
    "    context_pairs, gamma_values = [], []\n",
    "    nodes = list(graph.nodes)\n",
    "    if not nodes: return [], []\n",
    "    for _ in range(num_samples):\n",
    "        node = np.random.choice(nodes)\n",
    "        neighbors = list(nx.single_source_shortest_path_length(graph, node, cutoff=2).keys())\n",
    "        if len(neighbors) > 1:\n",
    "            neighbor = np.random.choice([n for n in neighbors if n != node])\n",
    "            context_pairs.append((node, neighbor))\n",
    "            gamma_values.append(1 if (labels[node] == labels[neighbor]) else 0)\n",
    "    return context_pairs, gamma_values\n",
    "\n",
    "def context_loss(embedding, context_pairs, gamma_values):\n",
    "    loss = 0\n",
    "    device = embedding.device\n",
    "    if not context_pairs: return torch.tensor(0.0, device=device)\n",
    "    for (i, j), gamma in zip(context_pairs, gamma_values):\n",
    "        if i < embedding.shape[0] and j < embedding.shape[0]:\n",
    "            score = torch.dot(embedding[i], embedding[j])\n",
    "            gamma_tensor = torch.tensor(gamma, device=device, dtype=torch.float32)\n",
    "            loss += gamma_tensor * F.logsigmoid(score) + (1 - gamma_tensor) * F.logsigmoid(-score)\n",
    "    return -loss / len(context_pairs)\n",
    "\n",
    "def planetoid_gcn_training(model, data, G, device, lr, wd, epochs):\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss_s = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        context_pairs, gamma_values = sample_context(G, data.y.cpu())\n",
    "        loss_u = context_loss(model.get_embedding(), context_pairs, gamma_values)\n",
    "        loss = loss_s + 0.5 * loss_u\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return torch.exp(model(data))\n",
    "\n",
    "def csse_training(model, data, device, lr, wd, alr, epochs):\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    optimizer_w = optim.Adam(model.get_net_params(), lr=lr, weight_decay=wd)\n",
    "    optimizer_a = optim.Adam(model.get_arch_params(), lr=alr, weight_decay=0.0)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer_w.zero_grad()\n",
    "        loss_w = F.nll_loss(model(data)[data.train_mask], data.y[data.train_mask])\n",
    "        loss_w.backward()\n",
    "        optimizer_w.step()\n",
    "        optimizer_a.zero_grad()\n",
    "        loss_a = F.nll_loss(model(data)[data.val_mask], data.y[data.val_mask])\n",
    "        loss_a.backward()\n",
    "        optimizer_a.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return torch.exp(model(data))\n",
    "\n",
    "def sdmg_training_and_inference(model, data, device):\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    for _ in range(200): # Simplified pre-training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        H = model(data).detach()\n",
    "    clf = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=42, max_iter=200)\n",
    "    clf.fit(H[data.train_mask].cpu().numpy(), data.y[data.train_mask].cpu().numpy())\n",
    "    probs = clf.predict_proba(H.cpu().numpy())\n",
    "    return torch.from_numpy(probs).float()\n",
    "\n",
    "# ====================================================================\n",
    "# 3. SIMPLEX AND HOI HELPER FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "# modified_function.py\n",
    "def group_by_size(cliques):\n",
    "    if not cliques: return []\n",
    "    max_len = len(max(cliques, key=len)) if cliques else 0\n",
    "    grouped = [[] for _ in range(max_len)]\n",
    "    for c in cliques:\n",
    "        if len(c) > 0: grouped[len(c)-1].append(c)\n",
    "    return grouped\n",
    "\n",
    "def _calculate_variance(participation_counter, num_nodes):\n",
    "    if not participation_counter: return 0.0\n",
    "    freqs = np.array(list(participation_counter.values()))\n",
    "    all_freqs = np.concatenate([freqs, np.zeros(num_nodes - len(freqs))])\n",
    "    return np.var(all_freqs)\n",
    "\n",
    "def generate_all_simplex_types_greedy(G: nx.Graph, budget: int = 10000):\n",
    "    print(\"1/4: Finding Maximal Cliques...\")\n",
    "    maximal_cliques_list = [sorted(c) for c in nx.find_cliques(G)]\n",
    "    print(\"2/4: Generating All Cliques...\")\n",
    "    all_cliques_set = {tuple(sorted(sub)) for m in maximal_cliques_list for k in range(1, len(m) + 1) for sub in combinations(m, k)}\n",
    "    all_cliques_list = [list(c) for c in all_cliques_set]\n",
    "    print(f\"3/4: Building Balanced Simplices (Greedy, Budget={budget})...\")\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    aug_max_set = {tuple(c) for c in maximal_cliques_list}\n",
    "    node_part = Counter(n for c in aug_max_set for n in c)\n",
    "    current_var = _calculate_variance(node_part, num_nodes)\n",
    "    candidates = all_cliques_set - aug_max_set\n",
    "    for i in range(budget):\n",
    "        if not candidates: break\n",
    "        best_clique, min_var = None, current_var\n",
    "        for cand in candidates:\n",
    "            temp_part = node_part.copy(); temp_part.update(cand)\n",
    "            pot_var = _calculate_variance(temp_part, num_nodes)\n",
    "            if pot_var < min_var: min_var, best_clique = pot_var, cand\n",
    "        if best_clique:\n",
    "            aug_max_set.add(best_clique); candidates.remove(best_clique)\n",
    "            node_part.update(best_clique); current_var = min_var\n",
    "        else: break\n",
    "    balanced_list = [list(c) for c in aug_max_set]\n",
    "    print(\"4/4: Formatting final results...\")\n",
    "    return group_by_size(all_cliques_list), group_by_size(maximal_cliques_list), group_by_size(balanced_list)\n",
    "\n",
    "def get_x_known(data, train_mask, val_mask, num_classes):\n",
    "    mask = train_mask | val_mask\n",
    "    indices = mask.nonzero().squeeze().cpu().numpy()\n",
    "    if indices.ndim == 0: indices = np.array([indices.item()])\n",
    "    labels = data.y[indices].cpu().numpy()\n",
    "    one_hot = np.eye(num_classes, dtype=np.int64)[labels]\n",
    "    return np.hstack([indices[:, np.newaxis], one_hot])\n",
    "\n",
    "def precompute_hoi_coefficients(n_max, n_classes, device):\n",
    "    print(\"Pre-computing HOI coefficients...\")\n",
    "    fact_lookup = torch.tensor([math.factorial(i) for i in range(n_max + 2)], dtype=torch.float32, device=device)\n",
    "    mat = [[torch.eye(n_classes, device=device)[i].unsqueeze(-1) for i in range(n_classes)]]\n",
    "    for k in range(n_max):\n",
    "        next_mat = []\n",
    "        if not mat[k]: break\n",
    "        for j in range(n_classes):\n",
    "            for i in range(len(mat[k])):\n",
    "                next_mat.append(torch.cat([mat[0][j], mat[k][i]], dim=1))\n",
    "        mat.append(next_mat)\n",
    "    \n",
    "    coef = [torch.empty(0, device=device)]\n",
    "    for k in range(1, n_max + 2):\n",
    "        cvals = []\n",
    "        if k-1 < len(mat) and mat[k-1]:\n",
    "            for j in range(len(mat[k-1])):\n",
    "                row_sums = mat[k-1][j].sum(1)\n",
    "                row_fac = torch.prod(fact_lookup[row_sums.long()])\n",
    "                cvals.append(fact_lookup[k] / row_fac)\n",
    "        coef.append(torch.tensor(cvals, device=device))\n",
    "    return coef\n",
    "\n",
    "def prob_product(vectors):\n",
    "    res = vectors[0]\n",
    "    for v in vectors[1:]: res = torch.ger(res, v).flatten()\n",
    "    return res\n",
    "\n",
    "def generalized_outer_product(P, index_lists):\n",
    "    if not index_lists: return torch.empty(0, device=P.device)\n",
    "    return torch.stack([prob_product([P[idx] for idx in indices]) for indices in index_lists])\n",
    "\n",
    "# ====================================================================\n",
    "# 4. HOI MODEL AND TRAINING (WITH WEIGHT STRATEGY)\n",
    "# ====================================================================\n",
    "\n",
    "def objective_efficient(P, simplices, device, precomputed_coef, weight_strategy='constant'):\n",
    "    K_MAX, n_max = 5, len(simplices)\n",
    "    \n",
    "    if weight_strategy == 'linear':\n",
    "        clique_weight = torch.arange(1, n_max + 1, device=device).float()\n",
    "    elif weight_strategy == 'exponential':\n",
    "        clique_weight = torch.exp(torch.arange(n_max, device=device).float())\n",
    "    else: # constant\n",
    "        clique_weight = torch.ones(n_max, device=device)\n",
    "        \n",
    "    total_obj = 0.0\n",
    "    for i in range(1, min(n_max, K_MAX)):\n",
    "        clique_size = i + 1\n",
    "        if not simplices[i] or clique_size >= len(precomputed_coef) or precomputed_coef[clique_size].numel() == 0:\n",
    "            continue\n",
    "        prob_prod = generalized_outer_product(P, simplices[i])\n",
    "        coef_slice = precomputed_coef[clique_size]\n",
    "        if coef_slice.shape[0] == prob_prod.shape[1]:\n",
    "            term_sum = (coef_slice * prob_prod).sum()\n",
    "            total_obj += clique_weight[i] * term_sum\n",
    "    return total_obj\n",
    "\n",
    "class HOIModel(nn.Module):\n",
    "    def __init__(self, device, initial_data, x_known, precomputed_coef):\n",
    "        super(HOIModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.precomputed_coef = precomputed_coef\n",
    "        init_tensor = torch.tensor(initial_data, dtype=torch.float32)\n",
    "        self.n_V, self.n_L = init_tensor.shape\n",
    "        self.fixed_indices = torch.from_numpy(x_known[:, 0].astype(int))\n",
    "        self.fixed_params = torch.tensor(x_known[:, 1:], dtype=torch.float32)\n",
    "        mask = torch.ones(self.n_V, dtype=torch.bool)\n",
    "        mask[self.fixed_indices] = False\n",
    "        self.trainable_indices = torch.arange(self.n_V)[mask]\n",
    "        self.trainable_params = nn.Parameter(init_tensor[mask])\n",
    "\n",
    "    def forward(self, simplices, weight_strategy):\n",
    "        full_data = torch.zeros((self.n_V, self.n_L), device=self.device)\n",
    "        full_data[self.fixed_indices] = self.fixed_params.to(self.device)\n",
    "        full_data[self.trainable_indices] = self.trainable_params\n",
    "        soft_P = F.softmax(full_data, dim=1)\n",
    "        return objective_efficient(soft_P, simplices, self.device, self.precomputed_coef, weight_strategy)\n",
    "\n",
    "def HOI_training(epochs, device, simplices, initial_data, x_known, lr, precomputed_coef, weight_strategy):\n",
    "    model = HOIModel(device, initial_data, x_known, precomputed_coef).to(device)\n",
    "    optimizer = optim.Adam([model.trainable_params], lr=lr)\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(simplices, weight_strategy)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        full_data = torch.zeros((model.n_V, model.n_L), device=model.device)\n",
    "        full_data[model.fixed_indices] = model.fixed_params.to(model.device)\n",
    "        full_data[model.trainable_indices] = model.trainable_params\n",
    "        probs = F.softmax(full_data, dim=1)\n",
    "    return probs\n",
    "\n",
    "# ====================================================================\n",
    "# 5. MAIN EXPERIMENT EXECUTION BLOCK\n",
    "# ====================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Basic Setup ---\n",
    "    SEEDS = list(range(10))\n",
    "    GNN_CONFIG = {\n",
    "        'GAT': {'class': GAT, 'lr': 0.005, 'wd': 5e-4, 'epochs': 1000},\n",
    "        'GCN': {'class': GCN, 'lr': 0.01, 'wd': 5e-4, 'epochs': 400},\n",
    "        'SGC': {'class': SGC, 'lr': 0.1, 'wd': 5e-5, 'epochs': 200},\n",
    "        'PlanetoidGCN': {'class': PlanetoidGCN, 'lr': 0.01, 'wd': 5e-4, 'epochs': 400},\n",
    "        'MGNN': {'class': MGNN, 'lr': 0.01, 'wd': 5e-4, 'epochs': 200, 'params': {'d_hidden': 32, 'd_red': 8}},\n",
    "        'SDMG': {'class': SDMG, 'lr': 0.005, 'wd': 5e-4, 'epochs': 200}, # Simplified training\n",
    "        'CSSE': {'class': CSSE, 'lr': 1e-2, 'wd': 5e-4, 'epochs': 200, 'params': {'alr': 3e-4, 'hidden_dim': 128}}\n",
    "    }\n",
    "    \n",
    "    # --- Data Loading and Pre-computation (run once) ---\n",
    "    print(\"Loading Coauthor-Physics dataset...\")\n",
    "    dataset = Coauthor(root='dataset/Coauthor-Physics', name='Physics')\n",
    "    data = dataset[0]\n",
    "    num_nodes, num_features, num_classes = data.num_nodes, data.num_features, dataset.num_classes\n",
    "    \n",
    "    print(\"Generating simplices (this may take a while)...\")\n",
    "    G = to_networkx(data.cpu(), to_undirected=True, remove_self_loops=True)\n",
    "    general_s, maximal_s, balanced_s = generate_all_simplex_types_greedy(G)\n",
    "    \n",
    "    print(\"Pre-computing HOI coefficients...\")\n",
    "    precomputed_coef = precompute_hoi_coefficients(len(general_s), num_classes, device)\n",
    "    \n",
    "    # --- Results Storage ---\n",
    "    results_storage = []\n",
    "\n",
    "    # --- Main Loop ---\n",
    "    for gnn_name, config in GNN_CONFIG.items():\n",
    "        for seed in SEEDS:\n",
    "            print(f\"\\n--- Running: GNN={gnn_name}, Seed={seed} ---\")\n",
    "            set_seed(seed)\n",
    "\n",
    "            # Data Splitting\n",
    "            perm = torch.randperm(num_nodes)\n",
    "            train_idx = sample_per_class_fixed(data.y, perm[:int(num_nodes*0.6)], num_classes, 20)\n",
    "            val_idx = sample_per_class_fixed(data.y, perm[int(num_nodes*0.6):int(num_nodes*0.8)], num_classes, 100)\n",
    "            test_idx = perm[int(num_nodes*0.8):]\n",
    "            data.train_mask = torch.zeros(num_nodes, dtype=torch.bool); data.train_mask[train_idx] = True\n",
    "            data.val_mask = torch.zeros(num_nodes, dtype=torch.bool); data.val_mask[val_idx] = True\n",
    "            data.test_mask = torch.zeros(num_nodes, dtype=torch.bool); data.test_mask[test_idx] = True\n",
    "\n",
    "            # --- Stage 1: Raw GNN Training ---\n",
    "            current_device = 'cpu' if gnn_name == 'SGC' else device\n",
    "            model_class = config['class']\n",
    "            \n",
    "            if gnn_name == 'PlanetoidGCN':\n",
    "                gnn_model = model_class(num_features, num_classes, num_nodes)\n",
    "                raw_probs = planetoid_gcn_training(gnn_model, data, G, current_device, config['lr'], config['wd'], config['epochs'])\n",
    "            elif gnn_name == 'MGNN':\n",
    "                # Pre-computation for MGNN\n",
    "                adj_sp = to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
    "                adj_norm_sp = adj_sp + sp.eye(adj_sp.shape[0])\n",
    "                data.adj_norm = torch.from_numpy(adj_norm_sp.toarray()).float().to(current_device)\n",
    "                # Simplified motif adjs\n",
    "                A2 = adj_sp.dot(adj_sp); A3 = A2.dot(adj_sp)\n",
    "                data.motif_adjs = [torch.from_numpy(m.toarray()).float().to(current_device) for m in [A2, A3, A2.dot(A2)]]\n",
    "                gnn_model = model_class(num_features, config['params']['d_hidden'], num_classes)\n",
    "                raw_probs = standard_gnn_training(gnn_model, data, current_device, config['lr'], config['wd'], config['epochs'])\n",
    "            elif gnn_name == 'SDMG':\n",
    "                gnn_model = model_class(num_features)\n",
    "                raw_probs = sdmg_training_and_inference(gnn_model, data, current_device)\n",
    "            elif gnn_name == 'CSSE':\n",
    "                gnn_model = model_class(num_features, config['params']['hidden_dim'], num_classes)\n",
    "                raw_probs = csse_training(gnn_model, data, current_device, config['lr'], config['wd'], config['params']['alr'], config['epochs'])\n",
    "            else: # GAT, GCN, SGC\n",
    "                gnn_model = model_class(num_features, num_classes)\n",
    "                raw_probs = standard_gnn_training(gnn_model, data, current_device, config['lr'], config['wd'], config['epochs'])\n",
    "\n",
    "            raw_preds = torch.argmax(raw_probs, dim=1)\n",
    "            raw_metrics = calculate_all_metrics(raw_preds, raw_probs, data.y, data.test_mask)\n",
    "            results_storage.append({'gnn': gnn_name, 'seed': seed, 'type': 'Raw', 'weight': 'N/A', **raw_metrics})\n",
    "            print(f\"  Raw {gnn_name} Accuracy: {raw_metrics['acc']:.4f}\")\n",
    "\n",
    "            # --- Stage 2: HOI Post-processing ---\n",
    "            initial_data = raw_probs.detach().cpu().numpy()\n",
    "            x_known = get_x_known(data, data.train_mask, data.val_mask, num_classes)\n",
    "            \n",
    "            for simplex_name, simplices in [('General', general_s), ('Maximal', maximal_s), ('Balanced', balanced_s)]:\n",
    "                for weight_name in ['Constant', 'Linear', 'Exponential']:\n",
    "                    hoi_probs = HOI_training(20, device, simplices, initial_data, x_known, 0.1, precomputed_coef, weight_name.lower())\n",
    "                    hoi_preds = torch.argmax(hoi_probs, dim=1)\n",
    "                    hoi_metrics = calculate_all_metrics(hoi_preds, hoi_probs, data.y, data.test_mask)\n",
    "                    results_storage.append({'gnn': gnn_name, 'seed': seed, 'type': simplex_name, 'weight': weight_name, **hoi_metrics})\n",
    "\n",
    "            del gnn_model, raw_probs, raw_preds; gc.collect()\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Final Results Aggregation and Display ---\n",
    "    df = pd.DataFrame(results_storage)\n",
    "    summary = df.groupby(['gnn', 'type', 'weight']).agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Formatting for display\n",
    "    for metric in ['acc', 'f1', 'auc', 'kappa']:\n",
    "        summary[f'{metric}_str'] = summary.apply(lambda row: f\"{row[(metric, 'mean')]:.4f} ± {row[(metric, 'std')]:.4f}\", axis=1)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\" \" * 25 + \"FINAL COMPREHENSIVE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for metric in ['acc', 'f1', 'auc', 'kappa']:\n",
    "        print(f\"\\n--- METRIC: {metric.upper()} ---\")\n",
    "        pivot = summary.pivot_table(index=['gnn', 'type'], columns='weight', values=f'{metric}_str', aggfunc='first')\n",
    "        # Reorder columns for better readability\n",
    "        cols_order = [c for c in ['N/A', 'Constant', 'Linear', 'Exponential'] if c in pivot.columns]\n",
    "        pivot = pivot[cols_order]\n",
    "        print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f169376-9767-41d3-8910-3b2d2673c797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2927b8e-a06c-4fc7-82bd-0e2fffea04e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee206d8-acfd-4f7a-817f-2e4e11042c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a9eb9-3300-4b9e-88d5-b8a3e5b07867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
